---
category: "99-Other"
source_url: "https://support.claude.com/en/articles/8525154-claude-is-providing-incorrect-or-misleading-responses-what-s-going-on"
---


In an attempt to be a helpful assistant, Claude can occasionally produce responses that are incorrect or misleading. 

 

This is known as "hallucinating" information, and it’s a byproduct of some of the current limitations of frontier Generative AI models, like Claude. For example, in some subject areas, Claude might not have been trained on the most-up-to-date information and may get confused when prompted about current events. Another example is that Claude can display quotes that may look authoritative or sound convincing, but are not grounded in fact. In other words, Claude can write things that might look correct but are very mistaken. 

 

Users should not rely on Claude as a singular source of truth and should carefully scrutinize any high-stakes advice given by Claude. 

 

When working with web search results, users should review Claude's cited sources. Original websites may contain important context or details not included in Claude's synthesis. Additionally, the quality of Claude's responses depends on the underlying sources it references, so checking original content helps you identify any information that might be misinterpreted without the full context.

 

You can use the thumbs down button to let us know if a particular response was unhelpful, or write to us at feedback@anthropic.com with your thoughts or suggestions.

 

To learn more about how Anthropic’s technology works and our research on developing safer, steerable, and more reliable models, we recommend visiting: https://www.anthropic.com/research

Related Articles
Using Research on Claude
Getting Started with Claude for Financial Services
Claude for Financial Services Overview
Getting Started with Claude for Life Sciences
Using Claude in Chrome Safely